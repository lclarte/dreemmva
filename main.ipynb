{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbasecondab01880b39f9c4020a8d680b488ee4bca",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging) # see https://stackoverflow.com/a/21475297/1469195\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "import braindecode\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load nn model from braindecode\n",
    "from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n",
    "from braindecode.models.deep4 import Deep4Net\n",
    "from torch import nn\n",
    "from braindecode.torch_ext.util import set_random_seeds\n",
    "\n",
    "import core.data as data\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Started loading file data/x_train.h5\nFinished loading the file.\nStarted loading file data/y_train.csv\nFinished loading the file.\n"
    }
   ],
   "source": [
    "# load data \n",
    "x_loaded, y_loaded = data.load_x('data/x_train.h5'), data.load_y('data/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_weights(y):\n",
    "    class_sample_count = np.array(\\\n",
    "                [len(np.where(y == t)[0]) for t in range(2)])\n",
    "    return float(len(y)) / class_sample_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "shape :  (24974, 7, 500) (12866, 7, 500) (24974,) (12866,)\nweights : [1.28262544 4.53825186]\n"
    }
   ],
   "source": [
    "# convert y to categorical with np.eye(d)[y_loaded]\n",
    "# and flatten the 40 independent samples\n",
    "x, y = data.flatten_x(x_loaded), data.flatten_y(y_loaded, repeat=40)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.66)\n",
    "x_train, x_test = x_train.squeeze(), x_test.squeeze()\n",
    "\n",
    "# Only one value : 0 or 1  \n",
    "y_train, y_test = np.argmax(y_train, axis=1), np.argmax(y_test, axis=1)\n",
    "\n",
    "# get class weights\n",
    "weights = class_weights(y_train)\n",
    "\n",
    "print('shape : ', x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "print('weights :', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "\n",
    "cuda = False\n",
    "set_random_seeds(seed=20170629, cuda=cuda)\n",
    "n_classes = 2\n",
    "in_chans = x_train.shape[1]\n",
    "# final_conv_length = auto ensures we only get a single output in the time dimension\n",
    "model = Deep4Net(in_chans=in_chans, n_classes=n_classes,\n",
    "                        input_time_length=x_train.shape[2],\n",
    "                        final_conv_length='auto') \n",
    "\n",
    "from braindecode.torch_ext.optimizers import AdamW\n",
    "import torch.nn.functional as F\n",
    "#optimizer = AdamW(model.parameters(), lr=1*0.01, weight_decay=0.5*0.001) # these are good values for the deep model\n",
    "optimizer = AdamW(model.parameters(), lr=0.0625 * 0.01, weight_decay=0)\n",
    "criterion = lambda prediction, targets : F.nll_loss(prediction, targets, weight=torch.from_numpy(weights).float())\n",
    "model.compile(loss=criterion, optimizer=optimizer, iterator_seed=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2020-02-08 17:00:05,030 INFO : Run until first stop...\n2020-02-08 17:00:31,926 INFO : Epoch 0\n2020-02-08 17:00:31,927 INFO : train_loss                20.75311\n2020-02-08 17:00:31,927 INFO : valid_loss                19.00763\n2020-02-08 17:00:31,927 INFO : train_misclass            0.24782\n2020-02-08 17:00:31,931 INFO : valid_misclass            0.25019\n2020-02-08 17:00:31,932 INFO : runtime                   0.00000\n2020-02-08 17:00:31,933 INFO : \n2020-02-08 17:02:01,765 INFO : Time only for training updates: 89.83s\n2020-02-08 17:02:38,019 INFO : Epoch 1\n2020-02-08 17:02:38,020 INFO : train_loss                0.70720\n2020-02-08 17:02:38,022 INFO : valid_loss                0.70786\n2020-02-08 17:02:38,031 INFO : train_misclass            0.22872\n2020-02-08 17:02:38,032 INFO : valid_misclass            0.23154\n2020-02-08 17:02:38,033 INFO : runtime                   116.73493\n2020-02-08 17:02:38,034 INFO : \n2020-02-08 17:04:20,383 INFO : Time only for training updates: 102.35s\n2020-02-08 17:05:09,035 INFO : Epoch 2\n2020-02-08 17:05:09,036 INFO : train_loss                0.69795\n2020-02-08 17:05:09,036 INFO : valid_loss                0.69899\n2020-02-08 17:05:09,040 INFO : train_misclass            0.23084\n2020-02-08 17:05:09,040 INFO : valid_misclass            0.23271\n2020-02-08 17:05:09,041 INFO : runtime                   138.61850\n2020-02-08 17:05:09,041 INFO : \n2020-02-08 17:06:49,625 INFO : Time only for training updates: 100.58s\n2020-02-08 17:07:22,686 INFO : Epoch 3\n2020-02-08 17:07:22,687 INFO : train_loss                0.69331\n2020-02-08 17:07:22,688 INFO : valid_loss                0.69471\n2020-02-08 17:07:22,688 INFO : train_misclass            0.24129\n2020-02-08 17:07:22,689 INFO : valid_misclass            0.24343\n2020-02-08 17:07:22,689 INFO : runtime                   149.24120\n2020-02-08 17:07:22,690 INFO : \n2020-02-08 17:08:56,694 INFO : Time only for training updates: 94.00s\n2020-02-08 17:09:33,725 INFO : Epoch 4\n2020-02-08 17:09:33,725 INFO : train_loss                0.69671\n2020-02-08 17:09:33,726 INFO : valid_loss                0.69753\n2020-02-08 17:09:33,727 INFO : train_misclass            0.22928\n2020-02-08 17:09:33,727 INFO : valid_misclass            0.23061\n2020-02-08 17:09:33,728 INFO : runtime                   127.07604\n2020-02-08 17:09:33,730 INFO : \n2020-02-08 17:11:10,493 INFO : Time only for training updates: 96.76s\n2020-02-08 17:11:46,432 INFO : Epoch 5\n2020-02-08 17:11:46,435 INFO : train_loss                0.69643\n2020-02-08 17:11:46,436 INFO : valid_loss                0.69773\n2020-02-08 17:11:46,436 INFO : train_misclass            0.23280\n2020-02-08 17:11:46,437 INFO : valid_misclass            0.23675\n2020-02-08 17:11:46,437 INFO : runtime                   133.79417\n2020-02-08 17:11:46,438 INFO : \n"
    },
    {
     "data": {
      "text/plain": "<braindecode.experiments.experiment.Experiment at 0x7f96e714d450>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5, batch_size=64, scheduler='cosine'\n",
    "         ,validation_data=(x_test, y_test)\n",
    "         ,input_time_length = 450) # supercropsize for cropped training\n",
    "# Rk : here, 1 timestep = 1 / 250 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Started loading file data/x_test.h5\nFinished loading the file.\n"
    }
   ],
   "source": [
    "x_challenge = data.load_x('data/x_test.h5')\n",
    "x_challenge = data.flatten_x(x_challenge).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the predictions of all the indepedent trials to return one classification for each subject\n",
    "def average_predictions(predictions, nb_trials = 40):\n",
    "    # number of samples\n",
    "    n = int(len(predictions) / nb_trials)\n",
    "    avg_preds = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        sample_preds = predictions[i*nb_trials:(i+1)*nb_trials]\n",
    "        avg_preds[i] = int(np.mean(sample_preds) > 0.5)\n",
    "    return avg_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_challenge = model.predict_classes(x_challenge)\n",
    "y_challenge2 = average_predictions(y_challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_csv(y, file_name):\n",
    "    with open(file_name, 'w') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerow(['id', 'label'])\n",
    "        for i in range(len(y)):\n",
    "            writer.writerow([str(i), str(int(y[i]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv(y_challenge2, 'data/result.csv')"
   ]
  }
 ]
}