{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import importlib\n",
    "importlib.reload(logging) # see https://stackoverflow.com/a/21475297/1469195\n",
    "log = logging.getLogger()\n",
    "log.setLevel('INFO')\n",
    "import sys\n",
    "\n",
    "import core.data as data\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                     level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import braindecode\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load nn model from braindecode\n",
    "from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n",
    "from braindecode.models.deep4 import Deep4Net\n",
    "from torch import nn\n",
    "from braindecode.torch_ext.util import set_random_seeds\n",
    "\n",
    "import core.data as data\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started loading file data/x_train.h5\n",
      "Finished loading the file.\n",
      "Started loading file data/y_train.csv\n",
      "Finished loading the file.\n"
     ]
    }
   ],
   "source": [
    "# load data \n",
    "x_loaded, y_loaded = data.load_x('data/x_train.h5'), data.load_y('data/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  (24974, 7, 500) (12866, 7, 500) (24974,) (12866,)\n",
      "weights : [0.64276522 2.25112674]\n"
     ]
    }
   ],
   "source": [
    "# convert y to categorical with np.eye(d)[y_loaded]\n",
    "# and flatten the 40 independent samples\n",
    "x, y = data.flatten_x(x_loaded), data.flatten_y(y_loaded, repeat=40)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.66)\n",
    "x_train, x_test = x_train.squeeze(), x_test.squeeze()\n",
    "\n",
    "# Only one value : 0 or 1  \n",
    "y_train, y_test = np.argmax(y_train, axis=1), np.argmax(y_test, axis=1)\n",
    "\n",
    "# get class weights\n",
    "weights = data.class_weights(y_train)\n",
    "\n",
    "print('shape : ', x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "print('weights :', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "\n",
    "cuda = False\n",
    "set_random_seeds(seed=20170629, cuda=cuda)\n",
    "n_classes = 2\n",
    "in_chans = x_train.shape[1]\n",
    "input_time_length=x_train.shape[2]\n",
    "final_conv_length='auto'\n",
    "# final_conv_length = auto ensures we only get a single output in the time dimension\n",
    "#Â model = Deep4Net(in_chans=in_chans, n_classes=n_classes, input_time_length=input_time_length, final_conv_length=final_conv_length)\n",
    "model = ShallowFBCSPNet(in_chans=in_chans, n_classes=n_classes,\n",
    "                        input_time_length=input_time_length,\n",
    "                        final_conv_length=final_conv_length)\n",
    "\n",
    "from braindecode.torch_ext.optimizers import AdamW\n",
    "import torch.nn.functional as F\n",
    "#optimizer = AdamW(model.parameters(), lr=1*0.01, weight_decay=0.5*0.001) # these are good values for the deep model\n",
    "optimizer = AdamW(model.parameters(), lr=0.0625 * 0.01, weight_decay=0)\n",
    "criterion = lambda prediction, targets : F.nll_loss(prediction, targets, weight=torch.from_numpy(weights).float())\n",
    "model.compile(loss=criterion, optimizer=optimizer, iterator_seed=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-19 11:34:12,115 INFO : Run until first stop...\n",
      "2020-03-19 11:36:05,460 INFO : Epoch 0\n",
      "2020-03-19 11:36:05,469 INFO : train_loss                1.12878\n",
      "2020-03-19 11:36:05,472 INFO : valid_loss                1.13823\n",
      "2020-03-19 11:36:05,475 INFO : train_misclass            0.53372\n",
      "2020-03-19 11:36:05,478 INFO : valid_misclass            0.53093\n",
      "2020-03-19 11:36:05,480 INFO : runtime                   0.00000\n",
      "2020-03-19 11:36:05,482 INFO : \n",
      "2020-03-19 11:45:12,965 INFO : Time only for training updates: 547.48s\n",
      "2020-03-19 11:46:56,014 INFO : Epoch 1\n",
      "2020-03-19 11:46:56,020 INFO : train_loss                1.06002\n",
      "2020-03-19 11:46:56,021 INFO : valid_loss                1.06488\n",
      "2020-03-19 11:46:56,021 INFO : train_misclass            0.76167\n",
      "2020-03-19 11:46:56,031 INFO : valid_misclass            0.76325\n",
      "2020-03-19 11:46:56,036 INFO : runtime                   660.84841\n",
      "2020-03-19 11:46:56,037 INFO : \n",
      "2020-03-19 11:56:03,210 INFO : Time only for training updates: 547.17s\n",
      "2020-03-19 11:57:33,496 INFO : Epoch 2\n",
      "2020-03-19 11:57:33,500 INFO : train_loss                0.86057\n",
      "2020-03-19 11:57:33,504 INFO : valid_loss                0.86884\n",
      "2020-03-19 11:57:33,505 INFO : train_misclass            0.76768\n",
      "2020-03-19 11:57:33,509 INFO : valid_misclass            0.77266\n",
      "2020-03-19 11:57:33,512 INFO : runtime                   650.24594\n",
      "2020-03-19 11:57:33,513 INFO : \n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=64, scheduler='cosine'\n",
    "         ,validation_data=(x_test, y_test)\n",
    "         ,input_time_length = 450) # supercropsize for cropped training\n",
    "# Rk : here, 1 timestep = 1 / 250 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_challenge = data.load_x('data/x_test.h5')\n",
    "x_challenge = data.flatten_x(x_challenge).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the predictions of all the indepedent trials to return one classification for each subject\n",
    "def average_predictions(predictions, nb_trials = 40):\n",
    "    # number of samples\n",
    "    n = int(len(predictions) / nb_trials)\n",
    "    avg_preds = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        sample_preds = predictions[i*nb_trials:(i+1)*nb_trials]\n",
    "        avg_preds[i] = int(np.mean(sample_preds) > 0.5)\n",
    "    return avg_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_challenge = model.predict_classes(x_challenge)\n",
    "y_challenge2 = data.average_predictions(y_challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data import save_csv \n",
    "save_csv(y_challenge2, 'data/result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualise the neural network (recepive field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondab01880b39f9c4020a8d680b488ee4bca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
